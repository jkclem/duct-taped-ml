{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the f distribution from scipy.stats\n",
    "from scipy.stats import f, t\n",
    "import numpy as np\n",
    "from scipy.optimize import newton, minimize\n",
    "class LinearModel():\n",
    "    \"\"\"The Linear Model Class is the parent class to all linear models.\"\"\"\n",
    "    \n",
    "    def __init__(self, add_intercept=True):\n",
    "        \"\"\"\n",
    "        Initializes the class with a boolean indicating whether or not the\n",
    "        class needs to add a column of 1s to all feature matrices to fit an\n",
    "        intercept and an empty beta_hat vector that will hold the regression\n",
    "        model's coefficients.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        add_intercept : bool, optional\n",
    "            Tells the class if it needs to add a column of 1s in the first\n",
    "            column of any data set passed to it, for fitting or prediction. If\n",
    "            the user does not want to include an intercept in the model, or \n",
    "            has already included a column of 1s in the data set for the \n",
    "            intercept, this should be set to False. The default is True.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None.\n",
    "\n",
    "        \"\"\"\n",
    "        self.add_intercept = add_intercept\n",
    "        self.beta_hat = None\n",
    "        return\n",
    "    \n",
    "    def fit():\n",
    "        \"\"\"This method will be overwritten by each of its child classes \n",
    "        because the method of fitting the linear model will vary from\n",
    "        algorithm to algorithm.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def _predict(self, X):\n",
    "        \"\"\"This method predicts the response values of the input array, X, in \n",
    "        the scale the model is estimated in; e.g. a logistic model will return\n",
    "        predictions in log-odds. The columns of X must match the number of \n",
    "        columns on the array on which the model was fit. The ordering must be\n",
    "        identical as well for the predictions to mean anything.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : numpy ndarray\n",
    "            A n x m matrix, where the n rows represent observations and the m\n",
    "            columns represent features of the observations.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        numpy ndarray\n",
    "            Returns a numpy ndarray with n elements that are the predicted \n",
    "            values of the response for each observation in X.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        X_copy = self._add_intercept(X)\n",
    "        \n",
    "        # Return the predictions.\n",
    "        return np.matmul(X_copy, self.beta_hat)\n",
    "    \n",
    "    def _add_intercept(self, X):\n",
    "        # If this object needs to add an intercept to new data, add one.\n",
    "        if self.add_intercept == True:\n",
    "            # Create an array of 1s equal in length to the observations in X.\n",
    "            intercept_column = np.repeat(1, repeats=X.shape[0])\n",
    "            # Insert it at the 0-th column index.\n",
    "            X_copy = np.insert(X, 0, intercept_column, axis=1)\n",
    "        # Otherwise, just copy X.\n",
    "        else:\n",
    "            X_copy = X\n",
    "        \n",
    "        return X_copy\n",
    "    \n",
    "class LogisticRegression(LinearModel):\n",
    "    def __init__(self, add_intercept=True):\n",
    "        super().__init__()\n",
    "    \n",
    "    def _inv_logit(self, X):\n",
    "        \"\"\"\n",
    "        This method takes in a vector of coefficients for a logistic \n",
    "        regression model and a matrix of data and returns the probabilities of\n",
    "        belonging to the class 1 by first calculating the log-odds and \n",
    "        translating the log-odds to probabilities. It is used by the \n",
    "        _log_likelihood and predict_probabilities methods.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        beta : numpy array\n",
    "            A 1-D vector of coefficients in a logistic regression model.\n",
    "        X : numpy array\n",
    "            A 2-D matrix where rows represent observations and columns \n",
    "            represent variables.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        probabilities : numpy array\n",
    "            A 1-D vector of the probabilities associated from the logistic\n",
    "            regression.\n",
    "\n",
    "        \"\"\"\n",
    "        # Add an intercept if desired.\n",
    "        X = self._add_intercept(X)\n",
    "        # Calculate the numerator of the inverse logit transformation.\n",
    "        numerator = np.exp(np.matmul(X, self.beta_hat))\n",
    "        # Calculate the denominator of the inverse logit transformation.\n",
    "        denominator = 1 + np.exp(np.matmul(X, self.beta_hat))\n",
    "        # Calculate the probabilities.\n",
    "        probabilities =  numerator / denominator \n",
    "        \n",
    "        return probabilities\n",
    "    \n",
    "    def _log_likelihood(self, y, X):\n",
    "        \"\"\"\n",
    "        Overwrites the _log_likelihood method inherited from the RegressorMCMC\n",
    "        class to calculate the log-likelihood of the logistic regression\n",
    "        coefficients given binomially-distributed data. It is used in the\n",
    "        model fitting process.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y : numpy array\n",
    "            A 1-D vector of 0s and 1s representing the two classes.\n",
    "        X : numpy array\n",
    "            A 2-D matrix where rows represent observations and columns \n",
    "            represent variables.\n",
    "        beta : numpy array\n",
    "            A 1-D vector of coefficients in a logistic regression model.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        log_likelihood : float\n",
    "            The log-likelihood of the beta vector given the data.\n",
    "\n",
    "        \"\"\"\n",
    "        # Add an intercept if desired.\n",
    "        X = self._add_intercept(X)\n",
    "        # Calculate the log-likelihood of beta given the data.\n",
    "        log_likelihood = np.sum(y*np.log(self._inv_logit(self.beta_hat, X)) \n",
    "                               + (1-y)*np.log((1-self._inv_logit(self.beta_hat\n",
    "                                                                 ,X))))\n",
    "        \n",
    "        return log_likelihood\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # Add an intercept if desired.\n",
    "        X = self._add_intercept(X)\n",
    "        beta_start = np.repeat(0, X.shape[1])\n",
    "        \n",
    "        def _log_likelihood_(beta):\n",
    "            # Calculate the log-likelihood of beta given the data.\n",
    "            log_likelihood = np.sum(y*np.log(self._inv_logit(beta, X)) \n",
    "                                    + (1-y)*np.log((1-self._inv_logit(beta, \n",
    "                                                                      X))))\n",
    "        \n",
    "            return log_likelihood\n",
    "        \n",
    "        def _log_likelihood_optimize(*args):\n",
    "            beta = np.array(*args)\n",
    "            return _log_likelihood_(beta)\n",
    "        \n",
    "        newton(_log_likelihood_optimize)\n",
    "        pass\n",
    "    \n",
    "    def predict_probabilities(self, X):\n",
    "        \"\"\"\n",
    "        This method returns predictions of belonging to class 1 in \n",
    "        probabilities because the predict method will give predictions in \n",
    "        log-odds.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : numpy array\n",
    "            A 2-D matrix where rows represent observations and columns \n",
    "            represent variables.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        predicted_probabilities : numpy array\n",
    "            A 1-D array of the predicted probabilites of belonging to class 1.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        # Add an intercept if desired.\n",
    "        X = self._add_intercept(X)\n",
    "        \n",
    "        # Calculate the probability of each new observation belonging to \n",
    "        # class 1.\n",
    "        predicted_probabilities = self._inv_logit(X)\n",
    "            \n",
    "        return predicted_probabilities\n",
    "    \n",
    "    def predict_classes(self, X, boundary=0.5):\n",
    "        \"\"\"\n",
    "        This method predicts the class of new observations based on a decision \n",
    "        boundary for probability. If predicted probability >= boundary, it is\n",
    "        predicted to belong to class 1.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : numpy array\n",
    "            A 2-D matrix where rows represent observations and columns \n",
    "            represent variables.\n",
    "        boundary : float, optional\n",
    "            A float on the closed interval between 0 and 1 and is the minimum\n",
    "            predicted probability to classify a new observation of belonging \n",
    "            to class 1. The default is 0.5.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        predicted_classes : numpy array\n",
    "            DESCRIPTION.\n",
    "\n",
    "        \"\"\"\n",
    "        # Add an intercept if desired.\n",
    "        X = self._add_intercept(X)\n",
    "        # Predict the probabilities of belonging to class 1.\n",
    "        predicted_probabilities = self.predict_probabilities(X)\n",
    "        # Set predictions to 1 or 0 based on the decision boundary.\n",
    "        predicted_classes = np.where(predicted_probabilities >= boundary, 1, 0)\n",
    "        \n",
    "        return predicted_classes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _add_intercept(X, add_intercept=True):\n",
    "        # If this object needs to add an intercept to new data, add one.\n",
    "        if add_intercept == True:\n",
    "            # Create an array of 1s equal in length to the observations in X.\n",
    "            intercept_column = np.repeat(1, repeats=X.shape[0])\n",
    "            # Insert it at the 0-th column index.\n",
    "            X_copy = np.insert(X, 0, intercept_column, axis=1)\n",
    "        # Otherwise, just copy X.\n",
    "        else:\n",
    "            X_copy = X\n",
    "        \n",
    "        return X_copy\n",
    "\n",
    "def _log_inv_logit(beta, X):\n",
    "\n",
    "        # Calculate the numerator of the inverse logit transformation.\n",
    "        numerator = 1\n",
    "        # Calculate the denominator of the inverse logit transformation.\n",
    "        denominator = 1 + np.exp(-np.dot(X, beta))\n",
    "        # Calculate the probabilities.\n",
    "        log_probabilities =  np.log(numerator) - np.log(denominator) \n",
    "        \n",
    "        return log_probabilities\n",
    "\n",
    "def _log_likelihood_(beta, X, y):\n",
    "    # Calculate the log-likelihood of beta given the data.\n",
    "    log_likelihood = np.sum(y*_log_inv_logit(beta, X) \n",
    "                        + (1-y)*(1-_log_inv_logit(beta, X)))\n",
    "        \n",
    "    return log_likelihood\n",
    "        \n",
    "def _neg_log_likelihood(beta, X, y):\n",
    "    return -_log_likelihood_(beta, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "\n",
    "x, y = make_classification(n_samples=100, n_features=4,n_informative=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-232.95970304228368\n",
      "-232.95970304228368\n",
      "-197.92848858652718\n",
      "-290.46715651024374\n",
      "-362.79804175563663\n",
      "-400.57522441869935\n",
      "-420.3624802931966\n",
      "-418.1422279783878\n",
      "-452.22625197604634\n",
      "nan\n",
      "-452.22625197604634\n",
      "-1778.8516833248464\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-f90a3bdadf0f>:19: RuntimeWarning: overflow encountered in exp\n",
      "  denominator = 1 + np.exp(-np.dot(X, beta))\n",
      "<ipython-input-10-f90a3bdadf0f>:27: RuntimeWarning: invalid value encountered in multiply\n",
      "  log_likelihood = np.sum(y*_log_inv_logit(beta, X)\n",
      "<ipython-input-10-f90a3bdadf0f>:28: RuntimeWarning: invalid value encountered in multiply\n",
      "  + (1-y)*(1-_log_inv_logit(beta, X)))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "   direc: array([[1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 1.]])\n",
       "     fun: nan\n",
       " message: 'NaN result encountered.'\n",
       "    nfev: 129\n",
       "     nit: 2\n",
       "  status: 3\n",
       " success: False\n",
       "       x: array([-1750.30033402,     8.48855792,     5.15105792,     4.35075792,\n",
       "           5.26645792])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = _add_intercept(x)\n",
    "beta_start = np.repeat(0., X.shape[1])\n",
    "beta_start = np.array([0.1788, 3.3127, -0.0248, -0.8251, 0.0906])\n",
    "\n",
    "#calling minimizer with Powell's or BFGS method\n",
    "minimize(_neg_log_likelihood,\n",
    "         beta_start,\n",
    "         args=(X,y),\n",
    "         method='Powell')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.259704\n",
      "         Iterations: 35\n"
     ]
    },
    {
     "ename": "LinAlgError",
     "evalue": "Singular matrix",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-9f602a24a83a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# building the model and fitting the data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mlog_reg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLogit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_reg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\statsmodels\\discrete\\discrete_model.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, start_params, method, maxiter, full_output, disp, callback, **kwargs)\u001b[0m\n\u001b[0;32m   1961\u001b[0m     def fit(self, start_params=None, method='newton', maxiter=35,\n\u001b[0;32m   1962\u001b[0m             full_output=1, disp=1, callback=None, **kwargs):\n\u001b[1;32m-> 1963\u001b[1;33m         bnryfit = super().fit(start_params=start_params,\n\u001b[0m\u001b[0;32m   1964\u001b[0m                               \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1965\u001b[0m                               \u001b[0mmaxiter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmaxiter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\statsmodels\\discrete\\discrete_model.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, start_params, method, maxiter, full_output, disp, callback, **kwargs)\u001b[0m\n\u001b[0;32m    225\u001b[0m             \u001b[1;32mpass\u001b[0m  \u001b[1;31m# TODO: make a function factory to have multiple call-backs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 227\u001b[1;33m         mlefit = super().fit(start_params=start_params,\n\u001b[0m\u001b[0;32m    228\u001b[0m                              \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m                              \u001b[0mmaxiter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmaxiter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, start_params, method, maxiter, full_output, disp, fargs, callback, retall, skip_hessian, **kwargs)\u001b[0m\n\u001b[0;32m    532\u001b[0m             \u001b[0mHinv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcov_params_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxopt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretvals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    533\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'newton'\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfull_output\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 534\u001b[1;33m             \u001b[0mHinv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mretvals\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Hessian'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mnobs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    535\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mskip_hessian\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    536\u001b[0m             \u001b[0mH\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhessian\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxopt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36minv\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\linalg\\linalg.py\u001b[0m in \u001b[0;36minv\u001b[1;34m(a)\u001b[0m\n\u001b[0;32m    544\u001b[0m     \u001b[0msignature\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'D->D'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misComplexType\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m'd->d'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    545\u001b[0m     \u001b[0mextobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_linalg_error_extobj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_raise_linalgerror_singular\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 546\u001b[1;33m     \u001b[0mainv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_umath_linalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msignature\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mextobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    547\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mainv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    548\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\linalg\\linalg.py\u001b[0m in \u001b[0;36m_raise_linalgerror_singular\u001b[1;34m(err, flag)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_raise_linalgerror_singular\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLinAlgError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Singular matrix\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_raise_linalgerror_nonposdef\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLinAlgError\u001b[0m: Singular matrix"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "# building the model and fitting the data\n",
    "log_reg = sm.Logit(y, X).fit()\n",
    "\n",
    "print(log_reg.summary())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
