{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the f distribution from scipy.stats\n",
    "from scipy.stats import f, t\n",
    "import numpy as np\n",
    "from scipy.optimize import newton, minimize, fmin_tnc\n",
    "class LinearModel():\n",
    "    \"\"\"The Linear Model Class is the parent class to all linear models.\"\"\"\n",
    "    \n",
    "    def __init__(self, add_intercept=True):\n",
    "        \"\"\"\n",
    "        Initializes the class with a boolean indicating whether or not the\n",
    "        class needs to add a column of 1s to all feature matrices to fit an\n",
    "        intercept and an empty beta_hat vector that will hold the regression\n",
    "        model's coefficients.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        add_intercept : bool, optional\n",
    "            Tells the class if it needs to add a column of 1s in the first\n",
    "            column of any data set passed to it, for fitting or prediction. If\n",
    "            the user does not want to include an intercept in the model, or \n",
    "            has already included a column of 1s in the data set for the \n",
    "            intercept, this should be set to False. The default is True.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None.\n",
    "\n",
    "        \"\"\"\n",
    "        self.add_intercept = add_intercept\n",
    "        self.beta_hat = None\n",
    "        return\n",
    "    \n",
    "    def fit():\n",
    "        \"\"\"This method will be overwritten by each of its child classes \n",
    "        because the method of fitting the linear model will vary from\n",
    "        algorithm to algorithm.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def _predict(self, X):\n",
    "        \"\"\"This method predicts the response values of the input array, X, in \n",
    "        the scale the model is estimated in; e.g. a logistic model will return\n",
    "        predictions in log-odds. The columns of X must match the number of \n",
    "        columns on the array on which the model was fit. The ordering must be\n",
    "        identical as well for the predictions to mean anything.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : numpy ndarray\n",
    "            A n x m matrix, where the n rows represent observations and the m\n",
    "            columns represent features of the observations.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        numpy ndarray\n",
    "            Returns a numpy ndarray with n elements that are the predicted \n",
    "            values of the response for each observation in X.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        X_copy = self._add_intercept(X)\n",
    "        \n",
    "        # Return the predictions.\n",
    "        return np.matmul(X_copy, self.beta_hat)\n",
    "    \n",
    "    def _add_intercept(self, X):\n",
    "        # If this object needs to add an intercept to new data, add one.\n",
    "        if self.add_intercept == True:\n",
    "            # Create an array of 1s equal in length to the observations in X.\n",
    "            intercept_column = np.repeat(1, repeats=X.shape[0])\n",
    "            # Insert it at the 0-th column index.\n",
    "            X_copy = np.insert(X, 0, intercept_column, axis=1)\n",
    "        # Otherwise, just copy X.\n",
    "        else:\n",
    "            X_copy = X\n",
    "        \n",
    "        return X_copy\n",
    "    \n",
    "class LogisticRegression(LinearModel):\n",
    "    def __init__(self, add_intercept=True):\n",
    "        super().__init__()\n",
    "        return\n",
    "    \n",
    "    def _sigmoid(self, beta, X):\n",
    "\n",
    "        # Calculate the numerator of the inverse logit transformation.\n",
    "        numerator = 1\n",
    "        # Calculate the denominator of the inverse logit transformation.\n",
    "        denominator = 1 + np.exp(-np.dot(X, beta))\n",
    "        \n",
    "        return numerator/denominator\n",
    "\n",
    "    def _log_likelihood(self, beta, X, y):\n",
    "    \n",
    "        p_hat = self._sigmoid(beta, X)\n",
    "    \n",
    "        # Calculate the log-likelihood of beta given the data.\n",
    "        log_likelihood = np.sum(y*np.log(p_hat)\n",
    "                                + (1-y)*(1-np.log(p_hat)))\n",
    "        \n",
    "        return log_likelihood\n",
    "        \n",
    "    def _neg_log_likelihood(self, beta, X, y):\n",
    "        return -self._log_likelihood(beta, X, y)\n",
    "\n",
    "    def _gradient(self, beta, X, y):\n",
    "        p_hat = self._sigmoid(beta, X)\n",
    "        return np.dot(X.T, (p_hat-y))\n",
    "    \n",
    "    def fit(self, X, y, method=\"BFGS\", max_iter=5000):\n",
    "        \n",
    "        assert ((method == \"Newton-CG\") \n",
    "                | (method == \"BFGS\")), \"Valid methods are 'Newton-CG' and 'BFGS'\"\n",
    "        assert ((type(max_iter) == int)\n",
    "                & (max_iter > 0)), \"max_iter must be a postive integer\"\n",
    "        \n",
    "        # Add an intercept if desired.\n",
    "        X = self._add_intercept(X)\n",
    "        \n",
    "        # Initialize a beta vector at 0.\n",
    "        beta_start = np.repeat(0, X.shape[1])\n",
    "        \n",
    "        # Perform the optimization.\n",
    "        opt_object = minimize(self._neg_log_likelihood,\n",
    "                               beta_start,\n",
    "                               args=(X,y),\n",
    "                               jac=self._gradient,\n",
    "                               method=method, \n",
    "                               options = {\"maxiter\": max_iter})\n",
    "        print(opt_object[\"message\"])\n",
    "        \n",
    "        # Set the beta_hat with the optimal result.\n",
    "        self.beta_hat = opt_object[\"x\"]\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def predict_probabilities(self, X):\n",
    "        \"\"\"\n",
    "        This method returns predictions of belonging to class 1 in \n",
    "        probabilities because the predict method will give predictions in \n",
    "        log-odds.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : numpy array\n",
    "            A 2-D matrix where rows represent observations and columns \n",
    "            represent variables.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        predicted_probabilities : numpy array\n",
    "            A 1-D array of the predicted probabilites of belonging to class 1.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        # Add an intercept if desired.\n",
    "        X = self._add_intercept(X)\n",
    "        \n",
    "        # Calculate the probability of each new observation belonging to \n",
    "        # class 1.\n",
    "        predicted_probabilities = self._sigmoid(self.beta_hat, X)\n",
    "            \n",
    "        return predicted_probabilities\n",
    "    \n",
    "    def predict_classes(self, X, boundary=0.5):\n",
    "        \"\"\"\n",
    "        This method predicts the class of new observations based on a decision \n",
    "        boundary for probability. If predicted probability >= boundary, it is\n",
    "        predicted to belong to class 1.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : numpy array\n",
    "            A 2-D matrix where rows represent observations and columns \n",
    "            represent variables.\n",
    "        boundary : float, optional\n",
    "            A float on the closed interval between 0 and 1 and is the minimum\n",
    "            predicted probability to classify a new observation of belonging \n",
    "            to class 1. The default is 0.5.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        predicted_classes : numpy array\n",
    "            The predicted class.\n",
    "\n",
    "        \"\"\"\n",
    "        # Add an intercept if desired.\n",
    "        X = self._add_intercept(X)\n",
    "        # Predict the probabilities of belonging to class 1.\n",
    "        predicted_probabilities = self.predict_probabilities(X)\n",
    "        # Set predictions to 1 or 0 based on the decision boundary.\n",
    "        predicted_classes = np.where(predicted_probabilities >= boundary, 1, 0)\n",
    "        \n",
    "        return predicted_classes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Desired error not necessarily achieved due to precision loss.\n",
      "[ 1.57611946 -0.75633672 -0.66826826 -1.52988827]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x24f7e53bbe0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy.stats import bernoulli\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(1)\n",
    "\n",
    "cols = 4\n",
    "n = 100\n",
    "\n",
    "beta = np.random.normal(scale=1, size=cols)\n",
    "x = np.random.normal(size=(n, cols-1))\n",
    "\n",
    "X = np.insert(x, 0, np.repeat(1, n), axis=1)\n",
    "\n",
    "log_odds = np.matmul(X, beta) + np.random.normal(scale=0.1, size=n)\n",
    "probs = 1/(1+np.exp(-log_odds))\n",
    "\n",
    "my_bernoulli = bernoulli(p=probs)\n",
    "y = my_bernoulli.rvs()\n",
    "\n",
    "my_logistic = LogisticRegression()\n",
    "\n",
    "my_logistic.fit(x, y, method=\"BFGS\")\n",
    "print(my_logistic.beta_hat)\n",
    "plt.scatter(my_logistic.predict_probabilities(x), probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      fun: -40.090401153757796\n",
       " hess_inv: array([[ 0.11598357, -0.04453939, -0.03353057, -0.06439292],\n",
       "       [-0.04453939,  0.09205664,  0.01831651,  0.03339303],\n",
       "       [-0.03353057,  0.01831651,  0.07828293,  0.03442046],\n",
       "       [-0.06439292,  0.03339303,  0.03442046,  0.11687452]])\n",
       "      jac: array([ 2.82284010e-05, -1.22906488e-06,  1.10587645e-04, -6.45095079e-05])\n",
       "  message: 'Desired error not necessarily achieved due to precision loss.'\n",
       "     nfev: 48\n",
       "      nit: 13\n",
       "     njev: 36\n",
       "   status: 2\n",
       "  success: False\n",
       "        x: array([ 1.57611946, -0.75633672, -0.66826826, -1.52988827])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def _add_intercept(X, add_intercept=True):\n",
    "        # If this object needs to add an intercept to new data, add one.\n",
    "        if add_intercept == True:\n",
    "            # Create an array of 1s equal in length to the observations in X.\n",
    "            intercept_column = np.repeat(1, repeats=X.shape[0])\n",
    "            # Insert it at the 0-th column index.\n",
    "            X_copy = np.insert(X, 0, intercept_column, axis=1)\n",
    "        # Otherwise, just copy X.\n",
    "        else:\n",
    "            X_copy = X\n",
    "        \n",
    "        return X_copy\n",
    "\n",
    "def _sigmoid(beta, X):\n",
    "\n",
    "        # Calculate the numerator of the inverse logit transformation.\n",
    "        numerator = 1\n",
    "        # Calculate the denominator of the inverse logit transformation.\n",
    "        denominator = 1 + np.exp(-np.dot(X, beta))\n",
    "        \n",
    "        return numerator/denominator\n",
    "\n",
    "def _log_likelihood(beta, X, y):\n",
    "    \n",
    "    p_hat = _sigmoid(beta, X)\n",
    "    \n",
    "    # Calculate the log-likelihood of beta given the data.\n",
    "    log_likelihood = np.sum(y*np.log(p_hat)\n",
    "                            + (1-y)*(1-np.log(p_hat)))\n",
    "        \n",
    "    return log_likelihood\n",
    "        \n",
    "def _neg_log_likelihood(beta, X, y):\n",
    "    return -_log_likelihood(beta, X, y)\n",
    "\n",
    "def gradient(beta, X, y):\n",
    "    p_hat = _sigmoid(beta, X)\n",
    "    return np.dot(X.T, (p_hat-y))\n",
    "beta_start = np.random.normal(size=X.shape[1])\n",
    "beta_start = np.repeat(0, X.shape[1])\n",
    "\n",
    "#calling minimizer with Powell's or BFGS method\n",
    "obj = minimize(_neg_log_likelihood,\n",
    "         beta_start,\n",
    "         args=(X,y),\n",
    "         jac=gradient,\n",
    "         method='BFGS', \n",
    "         options = {'maxiter': 5000})\n",
    "obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.422031\n",
      "         Iterations 7\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   No. Observations:                  100\n",
      "Model:                          Logit   Df Residuals:                       96\n",
      "Method:                           MLE   Df Model:                            3\n",
      "Date:                Wed, 14 Apr 2021   Pseudo R-squ.:                  0.2991\n",
      "Time:                        22:06:27   Log-Likelihood:                -42.203\n",
      "converged:                       True   LL-Null:                       -60.215\n",
      "Covariance Type:            nonrobust   LLR p-value:                 7.401e-08\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          1.5761      0.348      4.523      0.000       0.893       2.259\n",
      "x1            -0.7563      0.314     -2.409      0.016      -1.372      -0.141\n",
      "x2            -0.6683      0.309     -2.164      0.030      -1.274      -0.063\n",
      "x3            -1.5299      0.354     -4.320      0.000      -2.224      -0.836\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "# building the model and fitting the data\n",
    "log_reg = sm.Logit(y, X).fit()\n",
    "\n",
    "print(log_reg.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
