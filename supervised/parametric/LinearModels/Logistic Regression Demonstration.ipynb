{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the f distribution from scipy.stats\n",
    "from scipy.stats import f, t\n",
    "import numpy as np\n",
    "from scipy.optimize import newton, minimize, fmin_tnc\n",
    "class LinearModel():\n",
    "    \"\"\"The Linear Model Class is the parent class to all linear models.\"\"\"\n",
    "    \n",
    "    def __init__(self, add_intercept=True):\n",
    "        \"\"\"\n",
    "        Initializes the class with a boolean indicating whether or not the\n",
    "        class needs to add a column of 1s to all feature matrices to fit an\n",
    "        intercept and an empty beta_hat vector that will hold the regression\n",
    "        model's coefficients.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        add_intercept : bool, optional\n",
    "            Tells the class if it needs to add a column of 1s in the first\n",
    "            column of any data set passed to it, for fitting or prediction. If\n",
    "            the user does not want to include an intercept in the model, or \n",
    "            has already included a column of 1s in the data set for the \n",
    "            intercept, this should be set to False. The default is True.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None.\n",
    "\n",
    "        \"\"\"\n",
    "        self.add_intercept = add_intercept\n",
    "        self.beta_hat = None\n",
    "        return\n",
    "    \n",
    "    def fit():\n",
    "        \"\"\"This method will be overwritten by each of its child classes \n",
    "        because the method of fitting the linear model will vary from\n",
    "        algorithm to algorithm.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def _predict(self, X):\n",
    "        \"\"\"This method predicts the response values of the input array, X, in \n",
    "        the scale the model is estimated in; e.g. a logistic model will return\n",
    "        predictions in log-odds. The columns of X must match the number of \n",
    "        columns on the array on which the model was fit. The ordering must be\n",
    "        identical as well for the predictions to mean anything.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : numpy ndarray\n",
    "            A n x m matrix, where the n rows represent observations and the m\n",
    "            columns represent features of the observations.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        numpy ndarray\n",
    "            Returns a numpy ndarray with n elements that are the predicted \n",
    "            values of the response for each observation in X.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        X_copy = self._add_intercept(X)\n",
    "        \n",
    "        # Return the predictions.\n",
    "        return np.matmul(X_copy, self.beta_hat)\n",
    "    \n",
    "    def _add_intercept(self, X):\n",
    "        # If this object needs to add an intercept to new data, add one.\n",
    "        if self.add_intercept == True:\n",
    "            # Create an array of 1s equal in length to the observations in X.\n",
    "            intercept_column = np.repeat(1, repeats=X.shape[0])\n",
    "            # Insert it at the 0-th column index.\n",
    "            X_copy = np.insert(X, 0, intercept_column, axis=1)\n",
    "        # Otherwise, just copy X.\n",
    "        else:\n",
    "            X_copy = X\n",
    "        \n",
    "        return X_copy\n",
    "    \n",
    "class LogisticRegression(LinearModel):\n",
    "    def __init__(self, add_intercept=True):\n",
    "        super().__init__()\n",
    "    \n",
    "    def _sigmoid(self, beta, X):\n",
    "\n",
    "        # Calculate the numerator of the inverse logit transformation.\n",
    "        numerator = 1\n",
    "        # Calculate the denominator of the inverse logit transformation.\n",
    "        denominator = 1 + np.exp(-np.dot(X, beta))\n",
    "        \n",
    "        return numerator/denominator\n",
    "\n",
    "    def _log_likelihood(self, beta, X, y):\n",
    "    \n",
    "        p_hat = _sigmoid(beta, X)\n",
    "    \n",
    "        # Calculate the log-likelihood of beta given the data.\n",
    "        log_likelihood = np.sum(y*np.log(p_hat)\n",
    "                                + (1-y)*(1-np.log(p_hat)))\n",
    "        \n",
    "        return log_likelihood\n",
    "        \n",
    "    def _neg_log_likelihoodself, (beta, X, y):\n",
    "        return -_log_likelihood(beta, X, y)\n",
    "\n",
    "    def gradient(self, beta, X, y):\n",
    "        p_hat = _sigmoid(beta, X)\n",
    "        return np.dot(X.T, (p_hat-y))\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        assert ((method == \"Newton-CG\") \n",
    "                |)\n",
    "        \n",
    "        # Add an intercept if desired.\n",
    "        X = self._add_intercept(X)\n",
    "        \n",
    "        # Initialize a beta vector at 0.\n",
    "        beta_start = np.repeat(0, X.shape[1])\n",
    "        \n",
    "        opt_object = minimize(_neg_log_likelihood,\n",
    "                               beta_start,\n",
    "                               args=(X,y),\n",
    "                               jac=gradient,\n",
    "                               method='BFGS', \n",
    "                               options = {'maxiter': 5000})\n",
    "        pass\n",
    "    \n",
    "    def predict_probabilities(self, X):\n",
    "        \"\"\"\n",
    "        This method returns predictions of belonging to class 1 in \n",
    "        probabilities because the predict method will give predictions in \n",
    "        log-odds.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : numpy array\n",
    "            A 2-D matrix where rows represent observations and columns \n",
    "            represent variables.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        predicted_probabilities : numpy array\n",
    "            A 1-D array of the predicted probabilites of belonging to class 1.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        # Add an intercept if desired.\n",
    "        X = self._add_intercept(X)\n",
    "        \n",
    "        # Calculate the probability of each new observation belonging to \n",
    "        # class 1.\n",
    "        predicted_probabilities = self._sigmoid(self.beta_hat, X)\n",
    "            \n",
    "        return predicted_probabilities\n",
    "    \n",
    "    def predict_classes(self, X, boundary=0.5):\n",
    "        \"\"\"\n",
    "        This method predicts the class of new observations based on a decision \n",
    "        boundary for probability. If predicted probability >= boundary, it is\n",
    "        predicted to belong to class 1.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : numpy array\n",
    "            A 2-D matrix where rows represent observations and columns \n",
    "            represent variables.\n",
    "        boundary : float, optional\n",
    "            A float on the closed interval between 0 and 1 and is the minimum\n",
    "            predicted probability to classify a new observation of belonging \n",
    "            to class 1. The default is 0.5.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        predicted_classes : numpy array\n",
    "            The predicted class.\n",
    "\n",
    "        \"\"\"\n",
    "        # Add an intercept if desired.\n",
    "        X = self._add_intercept(X)\n",
    "        # Predict the probabilities of belonging to class 1.\n",
    "        predicted_probabilities = self.predict_probabilities(X)\n",
    "        # Set predictions to 1 or 0 based on the decision boundary.\n",
    "        predicted_classes = np.where(predicted_probabilities >= boundary, 1, 0)\n",
    "        \n",
    "        return predicted_classes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _add_intercept(X, add_intercept=True):\n",
    "        # If this object needs to add an intercept to new data, add one.\n",
    "        if add_intercept == True:\n",
    "            # Create an array of 1s equal in length to the observations in X.\n",
    "            intercept_column = np.repeat(1, repeats=X.shape[0])\n",
    "            # Insert it at the 0-th column index.\n",
    "            X_copy = np.insert(X, 0, intercept_column, axis=1)\n",
    "        # Otherwise, just copy X.\n",
    "        else:\n",
    "            X_copy = X\n",
    "        \n",
    "        return X_copy\n",
    "\n",
    "def _sigmoid(beta, X):\n",
    "\n",
    "        # Calculate the numerator of the inverse logit transformation.\n",
    "        numerator = 1\n",
    "        # Calculate the denominator of the inverse logit transformation.\n",
    "        denominator = 1 + np.exp(-np.dot(X, beta))\n",
    "        \n",
    "        return numerator/denominator\n",
    "\n",
    "def _log_likelihood(beta, X, y):\n",
    "    \n",
    "    p_hat = _sigmoid(beta, X)\n",
    "    \n",
    "    # Calculate the log-likelihood of beta given the data.\n",
    "    log_likelihood = np.sum(y*np.log(p_hat)\n",
    "                            + (1-y)*(1-np.log(p_hat)))\n",
    "        \n",
    "    return log_likelihood\n",
    "        \n",
    "def _neg_log_likelihood(beta, X, y):\n",
    "    return -_log_likelihood(beta, X, y)\n",
    "\n",
    "def gradient(beta, X, y):\n",
    "    p_hat = _sigmoid(beta, X)\n",
    "    return np.dot(X.T, (p_hat-y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import bernoulli\n",
    "np.random.seed(1)\n",
    "\n",
    "cols = 4\n",
    "n = 100\n",
    "\n",
    "beta = np.random.normal(scale=1, size=cols)\n",
    "x = np.random.normal(size=(n, cols-1))\n",
    "X = _add_intercept(x)\n",
    "\n",
    "log_odds = np.matmul(X, beta) + np.random.normal(scale=0.1, size=n)\n",
    "probs = 1/(1+np.exp(-log_odds))\n",
    "\n",
    "my_bernoulli = bernoulli(p=probs)\n",
    "y = my_bernoulli.rvs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "beta_start = np.random.normal(size=X.shape[1])\n",
    "beta_start = np.repeat(0, X.shape[1])\n",
    "\n",
    "#calling minimizer with Powell's or BFGS method\n",
    "obj = minimize(_neg_log_likelihood,\n",
    "         beta_start,\n",
    "         args=(X,y),\n",
    "         jac=gradient,\n",
    "         method='BFGS', \n",
    "         options = {'maxiter': 5000})\n",
    "#opt_weights = fmin_tnc(func=cost_function, x0=theta, fprime=gradient, args=(x, y.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      fun: -40.090401153757796\n",
       " hess_inv: array([[ 0.11598357, -0.04453939, -0.03353057, -0.06439292],\n",
       "       [-0.04453939,  0.09205664,  0.01831651,  0.03339303],\n",
       "       [-0.03353057,  0.01831651,  0.07828293,  0.03442046],\n",
       "       [-0.06439292,  0.03339303,  0.03442046,  0.11687452]])\n",
       "      jac: array([ 2.82284010e-05, -1.22906488e-06,  1.10587645e-04, -6.45095079e-05])\n",
       "  message: 'Desired error not necessarily achieved due to precision loss.'\n",
       "     nfev: 48\n",
       "      nit: 13\n",
       "     njev: 36\n",
       "   status: 2\n",
       "  success: False\n",
       "        x: array([ 1.57611946, -0.75633672, -0.66826826, -1.52988827])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-40.090401171008054, 40.090401171008054)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta = np.array([ 1.57611946, -0.75633672, -0.66826826, -1.52988827])\n",
    "_log_likelihood(beta, X, y), _neg_log_likelihood(beta, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.422031\n",
      "         Iterations 7\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   No. Observations:                  100\n",
      "Model:                          Logit   Df Residuals:                       96\n",
      "Method:                           MLE   Df Model:                            3\n",
      "Date:                Tue, 13 Apr 2021   Pseudo R-squ.:                  0.2991\n",
      "Time:                        23:26:10   Log-Likelihood:                -42.203\n",
      "converged:                       True   LL-Null:                       -60.215\n",
      "Covariance Type:            nonrobust   LLR p-value:                 7.401e-08\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          1.5761      0.348      4.523      0.000       0.893       2.259\n",
      "x1            -0.7563      0.314     -2.409      0.016      -1.372      -0.141\n",
      "x2            -0.6683      0.309     -2.164      0.030      -1.274      -0.063\n",
      "x3            -1.5299      0.354     -4.320      0.000      -2.224      -0.836\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "# building the model and fitting the data\n",
    "log_reg = sm.Logit(y, X).fit()\n",
    "\n",
    "print(log_reg.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
