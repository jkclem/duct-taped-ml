{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the f distribution from scipy.stats\n",
    "from scipy.stats import f, t\n",
    "import numpy as np\n",
    "from scipy.optimize import newton, minimize, fmin_tnc\n",
    "class LinearModel():\n",
    "    \"\"\"The Linear Model Class is the parent class to all linear models.\"\"\"\n",
    "    \n",
    "    def __init__(self, add_intercept=True):\n",
    "        \"\"\"\n",
    "        Initializes the class with a boolean indicating whether or not the\n",
    "        class needs to add a column of 1s to all feature matrices to fit an\n",
    "        intercept and an empty beta_hat vector that will hold the regression\n",
    "        model's coefficients.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        add_intercept : bool, optional\n",
    "            Tells the class if it needs to add a column of 1s in the first\n",
    "            column of any data set passed to it, for fitting or prediction. If\n",
    "            the user does not want to include an intercept in the model, or \n",
    "            has already included a column of 1s in the data set for the \n",
    "            intercept, this should be set to False. The default is True.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None.\n",
    "\n",
    "        \"\"\"\n",
    "        self.add_intercept = add_intercept\n",
    "        self.beta_hat = None\n",
    "        return\n",
    "    \n",
    "    def fit():\n",
    "        \"\"\"This method will be overwritten by each of its child classes \n",
    "        because the method of fitting the linear model will vary from\n",
    "        algorithm to algorithm.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def _predict(self, X):\n",
    "        \"\"\"This method predicts the response values of the input array, X, in \n",
    "        the scale the model is estimated in; e.g. a logistic model will return\n",
    "        predictions in log-odds. The columns of X must match the number of \n",
    "        columns on the array on which the model was fit. The ordering must be\n",
    "        identical as well for the predictions to mean anything.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : numpy ndarray\n",
    "            A n x m matrix, where the n rows represent observations and the m\n",
    "            columns represent features of the observations.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        numpy ndarray\n",
    "            Returns a numpy ndarray with n elements that are the predicted \n",
    "            values of the response for each observation in X.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        X_copy = self._add_intercept(X)\n",
    "        \n",
    "        # Return the predictions.\n",
    "        return np.matmul(X_copy, self.beta_hat)\n",
    "    \n",
    "    def _add_intercept(self, X):\n",
    "        # If this object needs to add an intercept to new data, add one.\n",
    "        if self.add_intercept == True:\n",
    "            # Create an array of 1s equal in length to the observations in X.\n",
    "            intercept_column = np.repeat(1, repeats=X.shape[0])\n",
    "            # Insert it at the 0-th column index.\n",
    "            X_copy = np.insert(X, 0, intercept_column, axis=1)\n",
    "        # Otherwise, just copy X.\n",
    "        else:\n",
    "            X_copy = X\n",
    "        \n",
    "        return X_copy\n",
    "    \n",
    "class LogisticRegression(LinearModel):\n",
    "    def __init__(self, add_intercept=True):\n",
    "        super().__init__()\n",
    "    \n",
    "    def _sigmoid(self, beta, X):\n",
    "\n",
    "        # Calculate the numerator of the inverse logit transformation.\n",
    "        numerator = 1\n",
    "        # Calculate the denominator of the inverse logit transformation.\n",
    "        denominator = 1 + np.exp(-np.dot(X, beta))\n",
    "        \n",
    "        return numerator/denominator\n",
    "\n",
    "    def _log_likelihood(self, beta, X, y):\n",
    "    \n",
    "        p_hat = _sigmoid(beta, X)\n",
    "    \n",
    "        # Calculate the log-likelihood of beta given the data.\n",
    "        log_likelihood = np.sum(y*np.log(p_hat)\n",
    "                                + (1-y)*(1-np.log(p_hat)))\n",
    "        \n",
    "        return log_likelihood\n",
    "        \n",
    "    def _neg_log_likelihood(self, beta, X, y):\n",
    "        return -_log_likelihood(beta, X, y)\n",
    "\n",
    "    def gradient(self, beta, X, y):\n",
    "        p_hat = _sigmoid(beta, X)\n",
    "        return np.dot(X.T, (p_hat-y))\n",
    "    \n",
    "    def fit(self, X, y, method=\"BFGS\", max_iter=5000):\n",
    "        \n",
    "        assert ((method == \"Newton-CG\") \n",
    "                | (method == \"BFGS\")), \"Valid methods are 'Newton-CG' and 'BFGS'\"\n",
    "        assert ((type(max_iter) == int)\n",
    "                & (max_iter > 0)), \"max_iter must be a postive integer\"\n",
    "        \n",
    "        # Add an intercept if desired.\n",
    "        X = self._add_intercept(X)\n",
    "        \n",
    "        # Initialize a beta vector at 0.\n",
    "        beta_start = np.repeat(0, X.shape[1])\n",
    "        \n",
    "        # Perform the optimization.\n",
    "        opt_object = minimize(self._neg_log_likelihood,\n",
    "                               beta_start,\n",
    "                               args=(X,y),\n",
    "                               jac=gradient,\n",
    "                               method=method, \n",
    "                               options = {\"maxiter\": max_iter})\n",
    "        print(opt_object[\"message\"])\n",
    "        \n",
    "        # Set the beta_hat with the optimal result.\n",
    "        self.beta_hat = opt_object[\"x\"]\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def predict_probabilities(self, X):\n",
    "        \"\"\"\n",
    "        This method returns predictions of belonging to class 1 in \n",
    "        probabilities because the predict method will give predictions in \n",
    "        log-odds.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : numpy array\n",
    "            A 2-D matrix where rows represent observations and columns \n",
    "            represent variables.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        predicted_probabilities : numpy array\n",
    "            A 1-D array of the predicted probabilites of belonging to class 1.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        # Add an intercept if desired.\n",
    "        X = self._add_intercept(X)\n",
    "        \n",
    "        # Calculate the probability of each new observation belonging to \n",
    "        # class 1.\n",
    "        predicted_probabilities = self._sigmoid(self.beta_hat, X)\n",
    "            \n",
    "        return predicted_probabilities\n",
    "    \n",
    "    def predict_classes(self, X, boundary=0.5):\n",
    "        \"\"\"\n",
    "        This method predicts the class of new observations based on a decision \n",
    "        boundary for probability. If predicted probability >= boundary, it is\n",
    "        predicted to belong to class 1.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : numpy array\n",
    "            A 2-D matrix where rows represent observations and columns \n",
    "            represent variables.\n",
    "        boundary : float, optional\n",
    "            A float on the closed interval between 0 and 1 and is the minimum\n",
    "            predicted probability to classify a new observation of belonging \n",
    "            to class 1. The default is 0.5.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        predicted_classes : numpy array\n",
    "            The predicted class.\n",
    "\n",
    "        \"\"\"\n",
    "        # Add an intercept if desired.\n",
    "        X = self._add_intercept(X)\n",
    "        # Predict the probabilities of belonging to class 1.\n",
    "        predicted_probabilities = self.predict_probabilities(X)\n",
    "        # Set predictions to 1 or 0 based on the decision boundary.\n",
    "        predicted_classes = np.where(predicted_probabilities >= boundary, 1, 0)\n",
    "        \n",
    "        return predicted_classes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Desired error not necessarily achieved due to precision loss.\n",
      "[ 0.32730606  0.32730606 -0.74643267 -0.62562096 -1.64844652]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x18b83f0e280>"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy.stats import bernoulli\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(1)\n",
    "\n",
    "cols = 4\n",
    "n = 100\n",
    "\n",
    "beta = np.random.normal(scale=1, size=cols)\n",
    "x = np.random.normal(size=(n, cols-1))\n",
    "X = _add_intercept(x)\n",
    "\n",
    "log_odds = np.matmul(X, beta) + np.random.normal(scale=0.1, size=n)\n",
    "probs = 1/(1+np.exp(-log_odds))\n",
    "\n",
    "my_bernoulli = bernoulli(p=probs)\n",
    "y = my_bernoulli.rvs()\n",
    "\n",
    "my_logistic = LogisticRegression(add_intercept=True)\n",
    "my_logistic.fit(X, y)\n",
    "print(my_logistic.beta_hat)\n",
    "plt.scatter(my_logistic.predict_probabilities(X), probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      fun: -40.090401153757796\n",
       " hess_inv: array([[ 0.11598357, -0.04453939, -0.03353057, -0.06439292],\n",
       "       [-0.04453939,  0.09205664,  0.01831651,  0.03339303],\n",
       "       [-0.03353057,  0.01831651,  0.07828293,  0.03442046],\n",
       "       [-0.06439292,  0.03339303,  0.03442046,  0.11687452]])\n",
       "      jac: array([ 2.82284010e-05, -1.22906488e-06,  1.10587645e-04, -6.45095079e-05])\n",
       "  message: 'Desired error not necessarily achieved due to precision loss.'\n",
       "     nfev: 48\n",
       "      nit: 13\n",
       "     njev: 36\n",
       "   status: 2\n",
       "  success: False\n",
       "        x: array([ 1.57611946, -0.75633672, -0.66826826, -1.52988827])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def _add_intercept(X, add_intercept=True):\n",
    "        # If this object needs to add an intercept to new data, add one.\n",
    "        if add_intercept == True:\n",
    "            # Create an array of 1s equal in length to the observations in X.\n",
    "            intercept_column = np.repeat(1, repeats=X.shape[0])\n",
    "            # Insert it at the 0-th column index.\n",
    "            X_copy = np.insert(X, 0, intercept_column, axis=1)\n",
    "        # Otherwise, just copy X.\n",
    "        else:\n",
    "            X_copy = X\n",
    "        \n",
    "        return X_copy\n",
    "\n",
    "def _sigmoid(beta, X):\n",
    "\n",
    "        # Calculate the numerator of the inverse logit transformation.\n",
    "        numerator = 1\n",
    "        # Calculate the denominator of the inverse logit transformation.\n",
    "        denominator = 1 + np.exp(-np.dot(X, beta))\n",
    "        \n",
    "        return numerator/denominator\n",
    "\n",
    "def _log_likelihood(beta, X, y):\n",
    "    \n",
    "    p_hat = _sigmoid(beta, X)\n",
    "    \n",
    "    # Calculate the log-likelihood of beta given the data.\n",
    "    log_likelihood = np.sum(y*np.log(p_hat)\n",
    "                            + (1-y)*(1-np.log(p_hat)))\n",
    "        \n",
    "    return log_likelihood\n",
    "        \n",
    "def _neg_log_likelihood(beta, X, y):\n",
    "    return -_log_likelihood(beta, X, y)\n",
    "\n",
    "def gradient(beta, X, y):\n",
    "    p_hat = _sigmoid(beta, X)\n",
    "    return np.dot(X.T, (p_hat-y))\n",
    "beta_start = np.random.normal(size=X.shape[1])\n",
    "beta_start = np.repeat(0, X.shape[1])\n",
    "\n",
    "#calling minimizer with Powell's or BFGS method\n",
    "obj = minimize(_neg_log_likelihood,\n",
    "         beta_start,\n",
    "         args=(X,y),\n",
    "         jac=gradient,\n",
    "         method='BFGS', \n",
    "         options = {'maxiter': 5000})\n",
    "obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.422031\n",
      "         Iterations 7\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   No. Observations:                  100\n",
      "Model:                          Logit   Df Residuals:                       96\n",
      "Method:                           MLE   Df Model:                            3\n",
      "Date:                Wed, 14 Apr 2021   Pseudo R-squ.:                  0.2991\n",
      "Time:                        22:06:27   Log-Likelihood:                -42.203\n",
      "converged:                       True   LL-Null:                       -60.215\n",
      "Covariance Type:            nonrobust   LLR p-value:                 7.401e-08\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          1.5761      0.348      4.523      0.000       0.893       2.259\n",
      "x1            -0.7563      0.314     -2.409      0.016      -1.372      -0.141\n",
      "x2            -0.6683      0.309     -2.164      0.030      -1.274      -0.063\n",
      "x3            -1.5299      0.354     -4.320      0.000      -2.224      -0.836\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "# building the model and fitting the data\n",
    "log_reg = sm.Logit(y, X).fit()\n",
    "\n",
    "print(log_reg.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
