{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression Regularization\n",
    "\n",
    "Linear models are typically thought of being models more prone to bias than variance when looking at the [bias-variance tradeoff](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff). However, when you have many features relative to your number of observations, linear models without regularization can be prone to overfitting and thus high bias. Regularization in the linear model context works by biasing the coefficients in the linear model towards zero. This increases variance, while increasing bias. The is accomplished by penalizing high magnitudes of coefficients. Choozing the optimal penalty to place on the magnitudes of coefficients is typically accomplished in [cross-validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)).\n",
    "\n",
    "First a refresher of basic linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Linear Regression Basics\n",
    "\n",
    "Linear regression is an often underrated tool in the machine learning toolkit (in my opinion). I cannot give linear models the treatment they deserve in this article, but I will give a brief overview. Linear models assume a model of the form:\n",
    "\n",
    "$y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2x _{i2} + ... + \\beta_k x_{ik} + \\epsilon_i$\n",
    "\n",
    "where $y_i$ is the label of the *i*-th observation, $\\hat{\\beta}_j$ is the coefficient for the *j*-th variable, $x_{ij}$ is the value for the *j*-th variable for the *i*-th observation, and $\\epsilon_i$ is the random error for the *i*-th observation.\n",
    "\n",
    "By far the most common loss function used in linear regression is the sum of squared residuals (SSR), which is just adding the squared error for each training observation. Linear regression using this loss function is called Ordinary Least Squares (OLS) regression. Minimizing SSR is equivalent to minimizing the mean squared error. Although the SSR is the most common loss function in linear regression, there are others. Two you should know are [Least Absolute Deviation](https://en.wikipedia.org/wiki/Least_absolute_deviations) and [Huber Loss](https://en.wikipedia.org/wiki/Huber_loss), which are less sensitive to outliers. Their details are not discussed here, but you should review them so you have them in your toolkit.\n",
    "\n",
    "Some assumptions about $\\epsilon_i$ make linear regression particularly useful for inference when fitting it when minimizing sum of squared residuals (SSR), which is equivalent to minimizing the mean squared error (MSE). These are the assumptions of [Gauss-Markov Theorem](https://en.wikipedia.org/wiki/Gauss%E2%80%93Markov_theorem). Again, I am not providing a deep dive on OLS here because it is too important to summarize in several paragraphs, but you should be aware of the distrubtional assumptions required to make inferences from OLS models. I'm just providing enough context to move onto regularization.\n",
    "\n",
    "## Solving for the coefficients in OLS.\n",
    "\n",
    "Our objective in matrix form is:\n",
    "\n",
    "# $\\text{min }_{\\hat{\\beta}} ((y - X\\hat{\\beta})^T(y - X\\hat{\\beta}))$\n",
    "\n",
    "where $y$ is the column vector of true values, $X$ is the design matrix, and $\\hat{\\beta}$ is the column vector of coefficient estimates in the linear model. A design matrix has rows representing observations and columns representing variables, typically the first column is all 1s for the intercept term.\n",
    "\n",
    "Using summation notation, the loss function is:\n",
    "\n",
    "# $\\text{min }_{\\hat{\\beta}} (\\Sigma_{i=1}^n (y_i - \\Sigma^k_{j=1} (\\hat{\\beta}_j x_{ij}))^2)$\n",
    "\n",
    "where $y_i$ is the label for the *i*-th observation, $\\hat{\\beta}_j$ is the estimated coefficient for the *j*-th variable, and $x_{ij}$ is the value for the  *j*-th variable for the *i*-th observation.\n",
    "\n",
    "When minimizing the SSR, we have an analytical solution called the normal equations:\n",
    "\n",
    "Expanding $(y - X\\hat{\\beta})^T(y - X\\hat{\\beta})$ we get:\n",
    "\n",
    "# $\\epsilon^T \\epsilon = y^Ty - 2\\hat{\\beta}^T X^T y + \\hat{\\beta}^TX^TX\\hat{\\beta}$\n",
    "\n",
    "Our loss function will be at a local minima (or maxima) when the first derivative with respect to the estimated coefficients is zero. Taking the first derivative of the loss with respect to the coefficients we have:\n",
    "\n",
    "# $\\frac{d \\epsilon^T \\epsilon}{d\\hat{\\beta}} = - 2 X^T y + 2 X^TX\\hat{\\beta}$\n",
    "\n",
    "Setting the derivative of our loss to zero we get:\n",
    "\n",
    "# $0 = - 2 X^T y + 2 X^TX\\hat{\\beta}$\n",
    "\n",
    "Now we can start solving for the coefficient vector. First we move the $-2 X^T y$ from the left side to the right.\n",
    "\n",
    "# $2 X^T y = 2 X^TX\\hat{\\beta}$\n",
    "\n",
    "Now we can divide away the $2$s.\n",
    "\n",
    "# $X^T y = X^TX\\hat{\\beta}$\n",
    "\n",
    "Then we can isolate $\\hat{\\beta}$ by multiplying both sides by the inverse of $X^TX$.\n",
    "\n",
    "# $(X^TX)^{-1}X^T y = \\hat{\\beta}$\n",
    "\n",
    "We now have the closed form solution for the coefficients that minimize the sum of squared residuals:\n",
    "\n",
    "# $\\hat{\\beta} = (X^TX)^{-1}X^T y$\n",
    "\n",
    "In practice, the coefficients in a linear model are not estimated this way directly. Inverting a matrix can be numerically unstable, so your favorite stats/machine learning library uses a method like [singular value decomposition](https://en.wikipedia.org/wiki/Singular_value_decomposition), or [QR decomposition](https://en.wikipedia.org/wiki/QR_decomposition) to estimate the coefficients in OLS. I used singular value decomposition in my implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from LinearModel import OLS, Ridge\n",
    "from helpers import make_datasets\n",
    "\n",
    "size = 2000\n",
    "cols = 3\n",
    "eta = 1\n",
    "\n",
    "X_train, y_train, X_test, y_test = make_datasets(size, cols, num_zero=1, beta_scale=1., eta=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model df: 3\n",
      "Residual df: 996\n",
      "R-squared: 0.9002\n",
      "Adj. R-squared: 0.8999\n",
      "F-stat: 2993.9391\n",
      "F-prob: 0.0\n",
      "Est. Coef.: [ 0.9893 -2.8303  0.7886 -0.035 ]\n",
      "Est. Coef. Std. Error: [0.0311 0.0315 0.0304 0.0319]\n",
      "t-stats: [ 31.8331 -89.7663  25.9239  -1.0985]\n",
      "P(|t-stat| > 0): [0.     0.     0.     0.2722]\n"
     ]
    }
   ],
   "source": [
    "my_ols = OLS()\n",
    "my_ols.fit(X_train, y_train)  \n",
    "\n",
    "print(f\"Model df: {my_ols.df_model}\")\n",
    "print(f\"Residual df: {my_ols.df_residuals}\")\n",
    "print(f\"R-squared: {round(my_ols.R_sq, 4)}\")\n",
    "print(f\"Adj. R-squared: {round(my_ols.adj_R_sq, 4)}\")\n",
    "print(f\"F-stat: {round(my_ols.F_stat, 4)}\")\n",
    "print(f\"F-prob: {round(my_ols.F_prob, 4)}\")\n",
    "print(f\"Est. Coef.: {np.round(my_ols.beta_hat, 4)}\")\n",
    "print(f\"Est. Coef. Std. Error: {np.round(my_ols.beta_hat_se, 4)}\")\n",
    "print(f\"t-stats: {np.round(my_ols.beta_hat_t_stats, 4)}\")\n",
    "print(f\"P(|t-stat| > 0): {np.round(my_ols.beta_hat_prob, 4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L1-Norm Regularization\n",
    "\n",
    "The least absolute shrinkage and selection operator, or the LASSO, performs both regularization and variable selection. When the penalty term is high enough some or all coefficients can be zero. The LASSO is escpecially useful when there are many variables and only a few are likely to be related to the quanitity being predicted.\n",
    "\n",
    "The loss function to be minimized is:\n",
    "\n",
    "# $\\text{min }_{\\hat{\\beta}} (\\Sigma_{i=1}^n (y_i - \\Sigma^k_{j=1} (\\hat{\\beta}_j x_{ij}))^2 + \\lambda \\Sigma_{l=1}^k |\\hat{\\beta}_l|)$\n",
    "\n",
    "Alternatively, it can be written as a constrained optimization problem:\n",
    "\n",
    "# $\\text{min }_{\\hat{\\beta}} (\\Sigma_{i=1}^n (y_i - \\Sigma^k_{j=1} (\\hat{\\beta}_j x_{ij}))^2)$, s.t. $t \\geq \\Sigma_{l=1}^k |\\hat{\\beta}_l|)$\n",
    "\n",
    "where $t$ is like a budget for how large the sum of the absolute values of the coefficients can be.\n",
    "\n",
    "The absolute value operation is not differentiable and the loss function has no closed form solution. Therefore it needs to be solved numerically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-1.09066714e-02, -2.07968766e+00,  1.15639693e-01, -4.96379663e-02,\n",
       "       -1.27491946e+00, -6.86830178e-01,  1.57268458e-11,  4.69290108e-01,\n",
       "        4.70102298e-01, -6.41151680e-02])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from LinearModel import LASSO\n",
    "\n",
    "X_train, y_train, X_test, y_test = make_datasets(100, 10, num_zero=round(cols/1.5), beta_scale=1., eta=1)\n",
    "\n",
    "my_lasso = LASSO()\n",
    "my_lasso.fit(X_train, y_train, alpha=10.0, method='Powell')\n",
    "\n",
    "my_lasso.beta_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L2-Norm Regularization (Ridge Regression)\n",
    "\n",
    "Ridge regression performs both regularization by penalizing the sum of squared coefficients. Unlike the LASSO it does not set coefficients to zero when the penalty is high enough so it does not perform variable selection. Ridge regression is escpecially useful when there are many variables and many of them are likely to be related to the quanitity being predicted, but there is [multicollinearity](https://en.wikipedia.org/wiki/Multicollinearity#:~:text=Multicollinearity%20refers%20to%20a%20situation,equal%20to%201%20or%20%E2%88%921.) between some of the features.\n",
    "\n",
    "The loss function to be minimized is:\n",
    "\n",
    "# $\\text{min }_{\\hat{\\beta}} (\\Sigma_{i=1}^n (y_i - \\Sigma^k_{j=1} (\\hat{\\beta}_j x_{ij}))^2 + \\lambda \\Sigma_{l=1}^k (\\hat{\\beta}_l)^2)$\n",
    "\n",
    "Unlike the LASSO, ridge regression has a closed form solution that can be obtained by solving the normal equations adding in the sum of squared coefficients multiplied by the penalty.\n",
    "\n",
    "# $\\text{min }_{\\hat{\\beta}} ((y - X\\hat{\\beta})^T(y - X\\hat{\\beta}) + \\lambda \\hat{\\beta}^T\\hat{\\beta})$\n",
    "\n",
    "Following the steps to solve the normal equation gets you:\n",
    "\n",
    "# $\\hat{\\beta}_{\\text{ridge}} = (X^TX + \\lambda I_p)^{-1}X^T y$\n",
    "\n",
    "where $I_p$ is the $kxk$ identity matrix and $k$ is the number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = make_datasets(100, 5, num_zero=0, beta_scale=4., eta=1, seed=2)\n",
    "\n",
    "coef_1, coef_2, coef_3, coef_4, coef_5 = [], [], [], [], []\n",
    "\n",
    "alphas = np.arange(0., 1000., 10)\n",
    "for alpha in alphas:\n",
    "    my_ridge = Ridge()\n",
    "    my_ridge.fit(X_train, y_train, alpha=alpha) \n",
    "\n",
    "    for i in range(len(my_ridge.beta_hat)):\n",
    "        beta_hat = my_ridge.beta_hat[i]\n",
    "        if i == 0:\n",
    "            coef_1.append(beta_hat)\n",
    "        elif i == 1:\n",
    "            coef_2.append(beta_hat)\n",
    "        elif i == 2:\n",
    "            coef_3.append(beta_hat)\n",
    "        elif i == 3:\n",
    "            coef_4.append(beta_hat)\n",
    "        else:\n",
    "            coef_5.append(beta_hat)\n",
    "        \n",
    "plt.plot(alphas, coef_1, color=\"blue\", label=\"Coef 1\")\n",
    "plt.plot(alphas, coef_2, color=\"red\", label=\"Coef 2\")\n",
    "plt.plot(alphas, coef_3, color=\"green\", label=\"Coef 3\")\n",
    "plt.plot(alphas, coef_4, color=\"orange\", label=\"Coef 4\")\n",
    "plt.plot(alphas, coef_5, color=\"purple\", label=\"Coef 5\")\n",
    "plt.title(\"Coefficients vs. Alpha\")\n",
    "plt.xlabel(\"Alpha\")\n",
    "plt.ylabel(\"Est. Coefs.\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = 500\n",
    "eta = 1\n",
    "\n",
    "X_train, y_train, X_test, y_test = make_datasets(size, cols, num_zero=round(cols/1.5), beta_scale=0.2, eta=1)\n",
    "\n",
    "X_std = (X_train - np.mean(X_train, axis=0))/np.std(X_train, axis=0)\n",
    "X_std_test = (X_test - np.mean(X_train, axis=0))/np.std(X_train, axis=0)\n",
    "\n",
    "mse = []\n",
    "train_mse = []\n",
    "alphas = np.arange(0, 200+1, 10)\n",
    "for alpha in alphas:\n",
    "    my_ridge = Ridge()\n",
    "    my_ridge.fit(X_train, y_train, alpha=alpha) \n",
    "    training_mse = np.mean((y_train - my_ridge.predict(X_train))**2)\n",
    "    train_mse.append(training_mse)\n",
    "    test_mse = np.mean((y_test - my_ridge.predict(X_test))**2)\n",
    "    mse.append(test_mse)\n",
    "\n",
    "plt.plot(alphas, mse, color=\"blue\", label=\"Test MSE\")\n",
    "plt.plot(alphas, train_mse, color=\"green\", label=\"Train MSE\")\n",
    "plt.hlines(y=eta, xmin=np.min(alphas), xmax=np.max(alphas), color=\"red\", label=\"True MSE\")\n",
    "plt.scatter(alphas[np.argmin(mse)], mse[np.argmin(mse)], color=\"black\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.title(\"MSE vs. Alpha\")\n",
    "plt.xlabel(\"Alpha\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "I hope you now have a better understanding of regularization in linear models. Simple modifications to the loss function of OLS are all that's needed to fit models less prone to overfitting, which is usefull when the ratio of variables to observations is high. You also have an idea of how the different errors behave with respect to $\\lambda$. \n",
    "\n",
    "Bonus:\n",
    "\n",
    "[Elastic Net Regression](https://en.wikipedia.org/wiki/Elastic_net_regularization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
