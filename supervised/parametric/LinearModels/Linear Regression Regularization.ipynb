{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Linear Regression Basics\n",
    "\n",
    "Linear regression is a tool often underrated tool in the machine learning toolkit. I cannot give linear models the treatment they deserve in this article, but I will give a brief overview. Linear models assume a model of the form:\n",
    "\n",
    "$y_i = \\beta_0 + \\beta_1 x_{i1} + + \\beta_2x _{i2} + ... + \\beta_k x_{ik} + \\epsilon_i$\n",
    "\n",
    "where $y_i$ is the label of the *i*-th observation,  $\\hat{\\beta}_j$ is the coefficient for the *j*-th variable, $x_{ij}$ is the value for the *j*-th variable for the *i*-th observation, and $\\epsilon_i$ is the error for the *i*-th observation.\n",
    "\n",
    "Some assumptions on $\\epsilon_i$ make linear regression particularly useful.\n",
    "\n",
    "The loss function most often used in linear regression is the sum of squared residuals (SSR). Minimizing this value is equivalent to minimizing the mean squared error (MSE). There are alternatives such as [Least Absolute Deviation](https://en.wikipedia.org/wiki/Least_absolute_deviations) and [Huber Loss](https://en.wikipedia.org/wiki/Huber_loss), which are less sensitive to outliers. Their details are not discussed here, but you should review them so you have them in your toolkit.\n",
    "\n",
    "The loss function in matrix form is:\n",
    "\n",
    "# $\\text{min } (y - X\\hat{\\beta})^T(y - X\\hat{\\beta})$\n",
    "\n",
    "where $y$ is the column vector of true values, $X$ is the design matrix, and $\\hat{\\beta}$ is the column vector of coefficient estimates in the linear model. A design matrix has rows representing observations and columns representing variables, typically the first column is all 1s for the intercept term.\n",
    "\n",
    "Using summation notation, the loss function is:\n",
    "\n",
    "# $\\text{min } \\Sigma_{i=1}^n (y_i - \\Sigma^k_{j=1} (\\hat{\\beta}_j x_{ij}))^2$\n",
    "\n",
    "where $y_i$ is the label for the *i*-th observation, $\\hat{\\beta}_j$ is the estimated coefficient for the *j*-th variable, and $x_{ij}$ is the value for the  *j*-th variable for the *i*-th observation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from LinearModel import OLS\n",
    "\n",
    "size = 2000\n",
    "cols = 3\n",
    "eta = 1\n",
    "\n",
    "def make_datasets(size, cols, num_zero=0, beta_scale=1., intercept=1., eta=1., seed=1):\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    X = np.random.normal(loc=0, scale=1, size=(size,cols))\n",
    "    beta = np.random.normal(loc=0, scale=beta_scale, size=cols)\n",
    "    \n",
    "    if num_zero != 0.:\n",
    "        beta_indices = np.arange(0, beta.shape[0], 1)\n",
    "        zero_indices = np.random.choice(beta_indices, num_zero, replace=False)\n",
    "        beta[zero_indices] = 0\n",
    "    \n",
    "    y = intercept + np.matmul(X, beta) + np.random.normal(loc=0, \n",
    "                                                          scale=eta, \n",
    "                                                          size=size) \n",
    "    X_train = X[:round(size/2),:]\n",
    "    y_train = y[:round(size/2)]  \n",
    "    X_test = X[round(size/2):,:]\n",
    "    y_test = y[round(size/2):] \n",
    "    \n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "\n",
    "X_train, y_train, X_test, y_test = make_datasets(size, cols, num_zero=1, beta_scale=1., eta=1)\n",
    " \n",
    "\n",
    "\n",
    "# Create an array of 1s equal in length to the observations in X.\n",
    "intercept_column = np.repeat(1, repeats=X_train.shape[0])\n",
    "# Insert it at the 0-th column index.\n",
    "X_copy = np.insert(X_train, 0, intercept_column, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model df: 3\n",
      "Residual df: 996\n",
      "R-squared: 0.9002\n",
      "Adj. R-squared: 0.8999\n",
      "F-stat: 2993.9391\n",
      "F-prob: 0.0\n",
      "Est. Coef.: [ 0.9893 -2.8303  0.7886 -0.035 ]\n",
      "Est. Coef. Std. Error: [0.0311 0.0315 0.0304 0.0319]\n",
      "t-stats: [ 31.8331 -89.7663  25.9239  -1.0985]\n",
      "P(|t-stat| > 0): [0.     0.     0.     0.2722]\n"
     ]
    }
   ],
   "source": [
    "my_ols = OLS()\n",
    "my_ols.fit(X_train, y_train)  \n",
    "\n",
    "print(f\"Model df: {my_ols.df_model}\")\n",
    "print(f\"Residual df: {my_ols.df_residuals}\")\n",
    "print(f\"R-squared: {round(my_ols.R_sq, 4)}\")\n",
    "print(f\"Adj. R-squared: {round(my_ols.adj_R_sq, 4)}\")\n",
    "print(f\"F-stat: {round(my_ols.F_stat, 4)}\")\n",
    "print(f\"F-prob: {round(my_ols.F_prob, 4)}\")\n",
    "print(f\"Est. Coef.: {np.round(my_ols.beta_hat, 4)}\")\n",
    "print(f\"Est. Coef. Std. Error: {np.round(my_ols.beta_hat_se, 4)}\")\n",
    "print(f\"t-stats: {np.round(my_ols.beta_hat_t_stats, 4)}\")\n",
    "print(f\"P(|t-stat| > 0): {np.round(my_ols.beta_hat_prob, 4)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
